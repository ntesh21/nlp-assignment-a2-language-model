{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PREDATOR\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\PREDATOR\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\PREDATOR\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Harry Potter Corpus\n",
    "Source: https://github.com/ErikaJacobs/Harry-Potter-Text-Mining/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About Dataset\n",
    "\n",
    "The dataset is taken from the github repi of **ErikaJacobs**. In this repo ErikaJacobs has performed a **Text Analysis of the Harry Potter Book Series**. \n",
    " - \"This project features sentiment analysis conducted on the text of the Harry Potter book series by JK Rowling.\"\n",
    "\n",
    "This repository contains the Harry Potter dataset under the folder **Book Text**. The folder contains 7 txt files each containing the text from single chapter or books. The files are:\n",
    "   * HPBook1.txt\n",
    "   * HPBook2.txt\n",
    "   * HPBook3.txt\n",
    "   * HPBook4.txt\n",
    "   * HPBook5.txt\n",
    "   * HPBook6.txt\n",
    "   * HPBook7.txt\n",
    "\n",
    "These text files were last committed four years ago. The size of the files are varying from 450kb to 1.4mb depending on the content of each book.\n",
    "\n",
    "According to the repo, the source for this file is another github repo **bradleyboehmke** : https://github.com/bradleyboehmke/harrypotter\n",
    "\n",
    "The details for the data in this repo is mentioned as:\n",
    "------------------------------------------------------\n",
    "\n",
    "An R Package for J.K. Rowling's Harry Potter Series\n",
    "\n",
    "This package provides access to the full texts of the first seven Harry Potter books. The UTF-8 plain text for each novel was sourced from [Read Vampire Books](www.readbooksvampire.com) **however the website is not currently accessible**, processed a bit, and is ready for text analysis. Each text is in a character vector with each element representing a single chapter. The package contains:\n",
    "\n",
    "-   `philosophers_stone`: Harry Potter and the Philosophers Stone, published in 1997\n",
    "-   `chamber_of_secrets`: Harry Potter and the Chamber of Secrets, published in 1998\n",
    "-   `prisoner_of_azkaban`: Harry Potter and the Prisoner of Azkaban, published in 1999\n",
    "-   `goblet_of_fire`: Harry Potter and the Goblet of Fire, published in 2000\n",
    "-   `order_of_the_phoenix`: Harry Potter and the Order of the Phoenix, published in 2003\n",
    "-   `half_blood_prince`: Harry Potter and the Half-Blood Prince, published in 2005\n",
    "-   `deathly_hallows`: Harry Potter and the Deathly Hallows, published in 2007\n",
    "\n",
    "* For this assignment we will use \n",
    "   - (HPBook1.txt HPBook2.txt HPBook3.txt HPBook4.txt) as training data\n",
    "   - (HPBook5.txt HPBook6.txt) as validation data\n",
    "   - (HPBook7.txt) as test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59adc9e9fb2447bfbce64347a8d85e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376abe72e30c496293929b1c86108b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f46192c9cc46e6bf26683a0048ffec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"text\", data_files={\"train\": [f'corpus-dataset/HPBook{i}.txt' for i in range(1,5)], \"test\":  [f'corpus-dataset/HPBook{i}.txt' for i in range(5,7)], \"validation\": f\"corpus-dataset/HPBook{7}.txt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 95\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 68\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 37\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c74d5d40364f8b9d0290b110b41ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/95 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b09e9c7a5c4ca8a87b73c4d8bc1eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/68 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa723684a18749b8900eb7cf9908951d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['harry', 'looked', 'bemusedly', 'at', 'the', 'photograph', 'colin', 'was', 'brandishing', 'under', 'his', 'nose', '.', 'a', 'moving', ',', 'black-and-white', 'lockhart', 'was', 'tugging', 'hard', 'on', 'an', 'arm', 'harry', 'recognized', 'as', 'his', 'own', '.', 'he', 'was', 'pleased', 'to', 'see', 'that', 'his', 'photographic', 'self', 'was', 'putting', 'up', 'a', 'good', 'fight', 'and', 'refusing', 'to', 'be', 'dragged', 'into', 'view', '.', 'as', 'harry', 'watched', ',', 'lockhart', 'gave', 'up', 'and', 'slumped', ',', 'panting', ',', 'against', 'the', 'white', 'edge', 'of', 'the', 'picture', '.', '\\\\will', 'you', 'sign', 'it', '?', '\\\\', 'said', 'colin', 'eagerly', '.', '\\\\no', ',', '\\\\', 'said', 'harry', 'flatly', ',', 'glancing', 'around', 'to', 'check', 'that', 'the', 'room', 'was', 'really', 'deserted', '.', '\\\\sorry', ',', 'colin', ',', 'i', \"'\", 'm', 'in', 'a', 'hurry', '-', 'quidditch', 'practice', '-\\\\', 'he', 'climbed', 'through', 'the', 'portrait', 'hole', '.', '\\\\oh', ',', 'wow', '!', 'wait', 'for', 'me', '!', 'i', \"'\", 've', 'never', 'watched', 'a', 'quidditch', 'game', 'before', '!', '\\\\', 'colin', 'scrambled', 'through', 'the', 'hole', 'after', 'him', '.', '\\\\it', \"'\", 'll', 'be', 'really', 'boring', ',', '\\\\', 'harry', 'said', 'quickly', ',', 'but', 'colin', 'ignored', 'him', ',', 'his', 'face', 'shining', 'with', 'excitement', '.', '\\\\you', 'were', 'the', 'youngest', 'house', 'player', 'in', 'a', 'hundred', 'years', ',', 'weren', \"'\", 't', 'you', ',', 'harry', '?', 'weren', \"'\", 't', 'you', '?', '\\\\', 'said', 'colin', ',', 'trotting', 'alongside', 'him', '.', '\\\\you', 'must', 'be', 'brilliant', '.', 'i', \"'\", 've', 'never', 'flown', '.', 'is', 'it', 'easy', '?', 'is', 'that', 'your', 'own', 'broom', '?', 'is', 'that', 'the', 'best', 'one', 'there', 'is', '?', '\\\\', 'harry', 'didn', \"'\", 't', 'know', 'how', 'to', 'get', 'rid', 'of', 'him', '.', 'it', 'was', 'like', 'having', 'an', 'extremely', 'talkative', 'shadow', '.', '\\\\i', 'don', \"'\", 't', 'really', 'understand', 'quidditch', ',', '\\\\', 'said', 'colin', 'breathlessly', '.', '\\\\is', 'it', 'true', 'there', 'are', 'four', 'balls', '?', 'and', 'two', 'of', 'them', 'fly', 'around', 'trying', 'to', 'knock', 'people', 'off', 'their', 'brooms', '?', '\\\\', '\\\\yes', ',', '\\\\', 'said', 'harry', 'heavily', ',', 'resigned', 'to', 'explaining', 'the', 'complicated', 'rules', 'of', 'quidditch', '.', '\\\\they', \"'\", 're', 'called', 'bludgers', '.', 'there', 'are', 'two', 'beaters', '*106*', 'on', 'each', 'team', 'who', 'carry', 'clubs', 'to', 'beat', 'the', 'bludgers', 'away', 'from', 'their', 'side', '.', 'fred', 'and', 'george', 'weasley', 'are', 'the', 'gryffindor', 'beaters', '.', '\\\\', '\\\\and', 'what', 'are', 'the', 'other', 'balls', 'for', '?', '\\\\', 'colin', 'asked', ',', 'tripping', 'down', 'a', 'couple', 'of', 'steps', 'because', 'he', 'was', 'gazing', 'open-mouthed', 'at', 'harry', '.', '\\\\well', ',', 'the', 'quafe', '-', 'that', \"'\", 's', 'the', 'biggish', 'red', 'one', '-', 'is', 'the', 'one', 'that', 'scores', 'goals', '.', 'three', 'chasers', 'on', 'each', 'team', 'throw', 'the', 'quaffle', 'to', 'each', 'other', 'and', 'try', 'and', 'get', 'it', 'through', 'the', 'goal', 'posts', 'at', 'the', 'end', 'of', 'the', 'pitch', '-', 'they', \"'\", 're', 'three', 'long', 'poles', 'with', 'hoops', 'on', 'the', 'end', '.', '\\\\', '\\\\and', 'the', 'fourth', 'ball', '-\\\\', '\\\\-', 'is', 'the', 'golden', 'snitch', ',', '\\\\', 'said', 'harry', ',', '\\\\and', 'it', \"'\", 's', 'very', 'small', ',', 'very', 'fast', ',', 'and', 'difficult', 'to', 'catch', '.', 'but', 'that', \"'\", 's', 'what', 'the', 'seeker', \"'\", 's', 'got', 'to', 'do', ',', 'because', 'a', 'game', 'of', 'quidditch', 'doesn', \"'\", 't', 'end', 'until', 'the', 'snitch', 'has', 'been', 'caught', '.', 'and', 'whichever', 'team', \"'\", 's', 'seeker', 'gets', 'the', 'snitch', 'earns', 'his', 'team', 'an', 'extra', 'hundred', 'and', 'fifty', 'points', '.', '\\\\', '\\\\and', 'you', \"'\", 're', 'the', 'gryffindor', 'seeker', ',', 'aren', \"'\", 't', 'you', '?', '\\\\', 'said', 'colin', 'in', 'awe', '.', '\\\\yes', ',', '\\\\', 'said', 'harry', 'as', 'they', 'left', 'the', 'castle', 'and', 'started', 'across', 'the', 'dew-', 'drenched', 'grass', '.', '\\\\and', 'there', \"'\", 's', 'the', 'keeper', ',', 'too', '.', 'he', 'guards', 'the', 'goal', 'posts', '.', 'that', \"'\", 's', 'it', ',', 'really', '.', '\\\\', 'but', 'colin', 'didn', \"'\", 't', 'stop', 'questioning', 'harry', 'all', 'the', 'way', 'down', 'the', 'sloping', 'lawns', 'to', 'the', 'quidditch', 'field', ',', 'and', 'harry', 'only', 'shook', 'him', 'off', 'when', 'he', 'reached', 'the', 'changing', 'rooms', 'colin', 'called', 'after', 'him', 'in', 'a', 'piping', 'voice', ',', '\\\\i', \"'\", 'll', 'go', 'and', 'get', 'a', 'good', 'seat', ',', 'harry', '!', '\\\\', 'and', 'hurried', 'off', 'to', 'the', 'stands', '.', 'the', 'rest', 'of', 'the', 'gryffindor', 'team', 'were', 'already', 'in', 'the', 'changing', 'room', '.', 'wood', 'was', 'the', 'only', 'person', 'who', 'looked', 'truly', 'awake', '.', 'fred', 'and', 'george', 'weasley', 'were', 'sitting', ',', 'puffy-eyed', 'and', 'touslehaired', ',', 'next', 'to', 'fourth', 'year', 'alicia', 'spinnet', ',', 'who', 'seemed', 'to', 'be', 'nodding', 'off', 'against', 'the', 'wall', 'behind', 'her', '.', 'her', 'fellow', 'chasers', ',', 'katie', '*107*', 'bell', 'and', 'angelina', 'johnson', ',', 'were', 'yawning', 'side', 'by', 'side', 'opposite', 'them', '.', '\\\\there', 'you', 'are', ',', 'harry', ',', 'what', 'kept', 'you', '?', '\\\\', 'said', 'wood', 'briskly', '.', '\\\\now', ',', 'i', 'wanted', 'a', 'quick', 'talk', 'with', 'you', 'all', 'before', 'we', 'actually', 'get', 'onto', 'the', 'field', ',', 'because', 'i', 'spent', 'the', 'summer', 'devising', 'a', 'whole', 'new', 'training', 'program', ',', 'which', 'i', 'really', 'think', 'will', 'make', 'all', 'the', 'difference', '.', '.', '.', '.', 'wood', 'was', 'holding', 'up', 'a', 'large', 'diagram', 'of', 'a', 'quidditch', 'field', ',', 'on', 'which', 'were', 'drawn', 'many', 'lines', ',', 'arrows', ',', 'and', 'crosses', 'in', 'differentcolored', 'inks', '.', 'he', 'took', 'out', 'his', 'wand', ',', 'tapped', 'the', 'board', ',', 'and', 'the', 'arrows', 'began', 'to', 'wiggle', 'over', 'the', 'diagram', 'like', 'caterpillars', '.', 'as', 'wood', 'launched', 'into', 'a', 'speech', 'about', 'his', 'new', 'tactics', ',', 'fred', 'weasley', \"'\", 's', 'head', 'drooped', 'right', 'onto', 'alicia', 'spinnet', \"'\", 's', 'shoulder', 'and', 'he', 'began', 'to', 'snore', '.', 'the', 'first', 'board', 'took', 'nearly', 'twenty', 'minutes', 'to', 'explain', ',', 'but', 'there', 'was', 'another', 'board', 'under', 'that', ',', 'and', 'a', 'third', 'under', 'that', 'one', '.', 'harry', 'sank', 'into', 'a', 'stupor', 'as', 'wood', 'droned', 'on', 'and', 'on', '.', '\\\\so', ',', '\\\\', 'said', 'wood', ',', 'at', 'long', 'last', ',', 'jerking', 'harry', 'from', 'a', 'wistful', 'fantasy', 'about', 'what', 'he', 'could', 'be', 'eating', 'for', 'breakfast', 'at', 'this', 'very', 'moment', 'up', 'at', 'the', 'castle', '.', '\\\\is', 'that', 'clear', '?', 'any', 'questions', '?', '\\\\', '\\\\i', \"'\", 've', 'got', 'a', 'question', ',', 'oliver', ',', '\\\\', 'said', 'george', ',', 'who', 'had', 'woken', 'with', 'a', 'start', '.', '\\\\why', 'couldn', \"'\", 't', 'you', 'have', 'told', 'us', 'all', 'this', 'yesterday', 'when', 'we', 'were', 'awake', '?', '\\\\', 'wood', 'wasn', \"'\", 't', 'pleased', '.', '\\\\now', ',', 'listen', 'here', ',', 'you', 'lot', ',', '\\\\', 'he', 'said', ',', 'glowering', 'at', 'them', 'all', '.', '\\\\we', 'should', 'have', 'won', 'the', 'quidditch', 'cup', 'last', 'year', '.', 'we', \"'\", 're', 'easily', 'the', 'best', 'team', '.', 'but', 'unfortunately', '-owing', 'to', 'circumstances', 'beyond', 'our', 'control', '-', '\\\\', '*108*', 'harry', 'shifted', 'guiltily', 'in', 'his', 'seat', '.', 'he', 'had', 'been', 'unconscious', 'in', 'the', 'hospital', 'wing', 'for', 'the', 'final', 'match', 'of', 'the', 'previous', 'year', ',', 'meaning', 'that', 'gryffindor', 'had', 'been', 'a', 'player', 'short', 'and', 'had', 'suffered', 'their', 'worst', 'defeat', 'in', 'three', 'hundred', 'years', '.', 'wood', 'took', 'a', 'moment', 'to', 'regain', 'control', 'of', 'himself', '.', 'their', 'last', 'defeat', 'was', 'clearly', 'still', 'torturing', 'him', '.', '\\\\so', 'this', 'year', ',', 'we', 'train', 'harder', 'than', 'ever', 'before', '.', '.', '.', '.', 'okay', ',', 'let', \"'\", 's', 'go', 'and', 'put', 'our', 'new', 'theories', 'into', 'practice', '!', '\\\\', 'wood', 'shouted', ',', 'seizing', 'his', 'broomstick', 'and', 'leading', 'the', 'way', 'out', 'of', 'the', 'locker', 'rooms', '.', 'stifflegged', 'and', 'still', 'yawning', ',', 'his', 'team', 'followed', '.', 'they', 'had', 'been', 'in', 'the', 'locker', 'room', 'so', 'long', 'that', 'the', 'sun', 'was', 'up', 'completely', 'now', ',', 'although', 'remnants', 'of', 'mist', 'hung', 'over', 'the', 'grass', 'in', 'the', 'stadium', '.', 'as', 'harry', 'walked', 'onto', 'the', 'field', ',', 'he', 'saw', 'ron', 'and', 'hermione', 'sitting', 'in', 'the', 'stands', '.', '\\\\aren', \"'\", 't', 'you', 'finished', 'yet', '?', '\\\\', 'called', 'ron', 'incredulously', '.', '\\\\haven', \"'\", 't', 'even', 'started', ',', '\\\\', 'said', 'harry', ',', 'looking', 'jealously', 'at', 'the', 'toast', 'and', 'marmalade', 'ron', 'and', 'hermione', 'had', 'brought', 'out', 'of', 'the', 'great', 'hall', '.', '\\\\wood', \"'\", 's', 'been', 'teaching', 'us', 'new', 'moves', '.', '\\\\', 'he', 'mounted', 'his', 'broomstick', 'and', 'kicked', 'at', 'the', 'ground', ',', 'soaring', 'up', 'into', 'the', 'air', '.', 'the', 'cool', 'morning', 'air', 'whipped', 'his', 'face', ',', 'waking', 'him', 'far', 'more', 'effectively', 'than', 'wood', \"'\", 's', 'long', 'talk', '.', 'it', 'felt', 'wonderful', 'to', 'be', 'back', 'on', 'the', 'quidditch', 'field', '.', 'he', 'soared', 'right', 'around', 'the', 'stadium', 'at', 'full', 'speed', ',', 'racing', 'fred', 'and', 'george', '.', '\\\\what', \"'\", 's', 'that', 'funny', 'clicking', 'noise', '?', '\\\\', 'called', 'fred', 'as', 'they', 'hurtled', 'around', 'the', 'corner', '.', 'harry', 'looked', 'into', 'the', 'stands', '.', 'colin', 'was', 'sitting', 'in', 'one', 'of', 'the', 'highest', 'seats', ',', 'his', 'camera', 'raised', ',', 'taking', 'picture', 'after', 'picture', ',', 'the', 'sound', 'strangely', 'magnified', 'in', 'the', 'deserted', 'stadium', '.', '*io9*', '\\\\look', 'this', 'way', ',', 'harry', '!', 'this', 'way', '!', '\\\\', 'he', 'cried', 'shrilly', '.', '\\\\who', \"'\", 's', 'that', '?', '\\\\', 'said', 'fred', '.', '\\\\no', 'idea', ',', '\\\\', 'harry', 'lied', ',', 'putting', 'on', 'a', 'spurt', 'of', 'speed', 'that', 'took', 'him', 'as', 'far', 'away', 'as', 'possible', 'from', 'colin', '.', '\\\\what', \"'\", 's', 'going', 'on', '?', '\\\\', 'said', 'wood', ',', 'frowning', ',', 'as', 'he', 'skimmed', 'through', 'the', 'air', 'toward', 'them', '.', '\\\\why', \"'\", 's', 'that', 'first', 'year', 'taking', 'pictures', '?', 'i', 'don', \"'\", 't', 'like', 'it', '.', 'he', 'could', 'be', 'a', 'slytherin', 'spy', ',', 'trying', 'to', 'find', 'out', 'about', 'our', 'new', 'training', 'program', '.', '\\\\', '\\\\he', \"'\", 's', 'in', 'gryffindor', ',', '\\\\', 'said', 'harry', 'quickly', '.', '\\\\and', 'the', 'slytherins', 'don', \"'\", 't', 'need', 'a', 'spy', ',', 'oliver', ',', '\\\\', 'said', 'george', '.', '\\\\what', 'makes', 'you', 'say', 'that', '?', '\\\\', 'said', 'wood', 'testily', '.', '\\\\because', 'they', \"'\", 're', 'here', 'in', 'person', ',', '\\\\', 'said', 'george', ',', 'pointing', '.', 'several', 'people', 'in', 'green', 'robes', 'were', 'walking', 'onto', 'the', 'field', ',', 'broomsticks', 'in', 'their', 'hands', '.', '\\\\i', 'don', \"'\", 't', 'believe', 'it', '!', '\\\\', 'wood', 'hissed', 'in', 'outrage', '.', '\\\\i', 'booked', 'the', 'field', 'for', 'today', '!', 'we', \"'\", 'll', 'see', 'about', 'this', '!', '\\\\', 'wood', 'shot', 'toward', 'the', 'ground', ',', 'landing', 'rather', 'harder', 'than', 'he', 'meant', 'to', 'in', 'his', 'anger', ',', 'staggering', 'slightly', 'as', 'he', 'dismounted', '.', 'harry', ',', 'fred', ',', 'and', 'george', 'followed', '.', '\\\\flint', '!', '\\\\', 'wood', 'bellowed', 'at', 'the', 'slytherin', 'captain', '.', '\\\\this', 'is', 'our', 'practice', 'time', '!', 'we', 'got', 'up', 'specially', '!', 'you', 'can', 'clear', 'off', 'now', '!', '\\\\', 'marcus', 'flint', 'was', 'even', 'larger', 'than', 'wood', '.', 'he', 'had', 'a', 'look', 'of', 'trollish', 'cunning', 'on', 'his', 'face', 'as', 'he', 'replied', ',', '\\\\plenty', 'of', 'room', 'for', 'all', 'of', 'us', ',', 'wood', '.', '\\\\', 'angelina', ',', 'alicia', ',', 'and', 'katie', 'had', 'come', 'over', ',', 'too', '.', 'there', 'were', 'no', 'girls', 'on', 'the', 'slytherin', 'team', ',', 'who', 'stood', 'shoulder', 'to', 'shoulder', ',', 'facing', 'the', 'gryffindors', ',', 'leering', 'to', 'a', 'man', '.', '\\\\but', 'i', 'booked', 'the', 'field', '!', '\\\\', 'said', 'wood', ',', 'positively', 'spitting', 'with', 'rage', '.', '\\\\i', 'booked', 'it', '!', '\\\\', '*110*', '\\\\ah', ',', '\\\\', 'said', 'flint', '.', '\\\\but', 'i', \"'\", 've', 'got', 'a', 'specially', 'signed', 'note', 'here', 'from', 'professor', 'snape', '.', '`i', ',', 'professor', 's', '.', 'snape', ',', 'give', 'the', 'slytherin', 'team', 'permission', 'to', 'practice', 'today', 'on', 'the', 'quidditch', 'field', 'owing', 'to', 'the', 'need', 'to', 'train', 'their', 'new', 'seeker', '.', '\\\\', \"'\", '\\\\you', \"'\", 've', 'got', 'a', 'new', 'seeker', '?', '\\\\', 'said', 'wood', ',', 'distracted', '.', '\\\\where', '?', '\\\\', 'and', 'from', 'behind', 'the', 'six', 'large', 'figures', 'before', 'them', 'came', 'a', 'seventh', ',', 'smaller', 'boy', ',', 'smirking', 'all', 'over', 'his', 'pale', ',', 'pointed', 'face', '.', 'it', 'was', 'draco', 'malfoy', '.', '\\\\aren', \"'\", 't', 'you', 'lucius', 'malfoy', \"'\", 's', 'son', '?', '\\\\', 'said', 'fred', ',', 'looking', 'at', 'malfoy', 'with', 'dislike', '.', '\\\\funny', 'you', 'should', 'mention', 'draco', \"'\", 's', 'father', ',', '\\\\', 'said', 'flint', 'as', 'the', 'whole', 'slytherin', 'team', 'smiled', 'still', 'more', 'broadly', '.', '\\\\let', 'me', 'show', 'you', 'the', 'generous', 'gift', 'he', \"'\", 's', 'made', 'to', 'the', 'slytherin', 'team', '.', '\\\\', 'all', 'seven', 'of', 'them', 'held', 'out', 'their', 'broomsticks', '.', 'seven', 'highly', 'polished', ',', 'brand-new', 'handles', 'and', 'seven', 'sets', 'of', 'fine', 'gold', 'lettering', 'spelling', 'the', 'words', 'nimbus', 'two', 'thousand', 'and', 'one', 'gleamed', 'under', 'the', 'gryffindors', \"'\", 'noses', 'in', 'the', 'early', 'morning', 'sun', '.', '\\\\very', 'latest', 'model', '.', 'only', 'came', 'out', 'last', 'month', ',', '\\\\', 'said', 'flint', 'carelessly', ',', 'flicking', 'a', 'speck', 'of', 'dust', 'from', 'the', 'end', 'of', 'his', 'own', '.', '\\\\i', 'believe', 'it', 'outstrips', 'the', 'old', 'two', 'thousand', 'series', 'by', 'a', 'considerable', 'amount', '.', 'as', 'for', 'the', 'old', 'cleansweeps\\\\', '-', 'he', 'smiled', 'nastily', 'at', 'fred', 'and', 'george', ',', 'who', 'were', 'both', 'clutching', 'cleansweep', 'fives', '-', '\\\\sweeps', 'the', 'board', 'with', 'them', '.', '\\\\', 'none', 'of', 'the', 'gryffindor', 'team', 'could', 'think', 'of', 'anything', 'to', 'say', 'for', 'a', 'moment', '.', 'malfoy', 'was', 'smirking', 'so', 'broadly', 'his', 'cold', 'eyes', 'were', 'reduced', 'to', 'slits', '.', '\\\\oh', ',', 'look', ',', '\\\\', 'said', 'flint', '.', '\\\\a', 'field', 'invasion', '.', '\\\\', 'ron', 'and', 'hermione', 'were', 'crossing', 'the', 'grass', 'to', 'see', 'what', 'was', 'going', 'on', '.', '*111*', '\\\\what', \"'\", 's', 'happening', '?', '\\\\', 'ron', 'asked', 'harry', '.', '\\\\why', 'aren', \"'\", 't', 'you', 'playing', '?', 'and', 'what', \"'\", 's', 'he', 'doing', 'here', '?', '\\\\', 'he', 'was', 'looking', 'at', 'malfoy', ',', 'taking', 'in', 'his', 'slytherin', 'quidditch', 'robes', '.', '\\\\i', \"'\", 'm', 'the', 'new', 'slytherin', 'seeker', ',', 'weasley', ',', '\\\\', 'said', 'malfoy', ',', 'smugly', '.', '\\\\everyone', \"'\", 's', 'just', 'been', 'admiring', 'the', 'brooms', 'my', 'father', \"'\", 's', 'bought', 'our', 'team', '.', 'ron', 'gaped', ',', 'open-mouthed', ',', 'at', 'the', 'seven', 'superb', 'broomsticks', 'in', 'front', 'of', 'him', '.', '\\\\good', ',', 'aren', \"'\", 't', 'they', '?', '\\\\', 'said', 'malfoy', 'smoothly', '.', '\\\\but', 'perhaps', 'the', 'gryffindor', 'team', 'will', 'be', 'able', 'to', 'raise', 'some', 'gold', 'and', 'get', 'new', 'brooms', ',', 'too', '.', 'you', 'could', 'raffle', 'off', 'those', 'cleansweep', 'fives', 'i', 'expect', 'a', 'museum', 'would', 'bid', 'for', 'them', '.', '\\\\', 'the', 'slytherin', 'team', 'howled', 'with', 'laughter', '.', '\\\\at', 'least', 'no', 'one', 'on', 'the', 'gryffindor', 'team', 'had', 'to', 'buy', 'their', 'way', 'in', ',', '\\\\', 'said', 'hermione', 'sharply', '.', '\\\\they', 'got', 'in', 'on', 'pure', 'talent', '.', '\\\\', 'the', 'smug', 'look', 'on', 'malfoy', \"'\", 's', 'face', 'flickered', '.', '\\\\no', 'one', 'asked', 'your', 'opinion', ',', 'you', 'fiithy', 'little', 'mudblood', ',', '\\\\', 'he', 'spat', '.', 'harry', 'knew', 'at', 'once', 'that', 'malfoy', 'had', 'said', 'something', 'really', 'bad', 'because', 'there', 'was', 'an', 'instant', 'uproar', 'at', 'his', 'words', '.', 'flint', 'had', 'to', 'dive', 'in', 'front', 'of', 'malfoy', 'to', 'stop', 'fred', 'and', 'george', 'jumping', 'on', 'him', ',', 'alicia', 'shrieked', ',', '\\\\how', 'dare', 'you', '!', '\\\\', 'and', 'ron', 'plunged', 'his', 'hand', 'into', 'his', 'robes', ',', 'pulled', 'out', 'his', 'wand', ',', 'yelling', ',', '\\\\you', \"'\", 'll', 'pay', 'for', 'that', 'one', ',', 'malfoy', '!', '\\\\', 'and', 'pointed', 'it', 'furiously', 'under', 'flint', \"'\", 's', 'arm', 'at', 'malfoys', 'face', '.', 'a', 'loud', 'bang', 'echoed', 'around', 'the', 'stadium', 'and', 'a', 'jet', 'of', 'green', 'light', 'shot', 'out', 'of', 'the', 'wrong', 'end', 'of', 'ron', \"'\", 's', 'wand', ',', 'hitting', 'him', 'in', 'the', 'stomach', 'and', 'sending', 'him', 'reeling', 'backward', 'onto', 'the', 'grass', '.', '12', '\\\\ron', '!', 'ron', '!', 'are', 'you', 'all', 'right', '?', '\\\\', 'squealed', 'hermione', '.', 'ron', 'opened', 'his', 'mouth', 'to', 'speak', ',', 'but', 'no', 'words', 'came', 'out', '.', 'instead', 'he', 'gave', 'an', 'almighty', 'belch', 'and', 'several', 'slugs', 'dribbled', 'out', 'of', 'his', 'mouth', 'onto', 'his', 'lap', '.', 'the', 'slytherin', 'team', 'were', 'paralyzed', 'with', 'laughter', '.', 'flint', 'was', 'doubled', 'up', ',', 'hanging', 'onto', 'his', 'new', 'broomstick', 'for', 'support', '.', 'malfoy', 'was', 'on', 'all', 'fours', ',', 'banging', 'the', 'ground', 'with', 'his', 'fist', '.', 'the', 'gryffindors', 'were', 'gathered', 'around', 'ron', ',', 'who', 'kept', 'belching', 'large', ',', 'glistening', 'slugs', '.', 'nobody', 'seemed', 'to', 'want', 'to', 'touch', 'him', '.', '\\\\we', \"'\", 'd', 'better', 'get', 'him', 'to', 'hagrid', \"'\", 's', ',', 'it', \"'\", 's', 'nearest', ',', '\\\\', 'said', 'harry', 'to', 'hermione', ',', 'who', 'nodded', 'bravely', ',', 'and', 'the', 'pair', 'of', 'them', 'pulled', 'ron', 'up', 'by', 'the', 'arms', '.', '\\\\what', 'happened', ',', 'harry', '?', 'what', 'happened', '?', 'is', 'he', 'ill', '?', 'but', 'you', 'can', 'cure', 'him', ',', 'can', \"'\", 't', 'you', '?', '\\\\', 'colin', 'had', 'run', 'down', 'from', 'his', 'seat', 'and', 'was', 'now', 'dancing', 'alongside', 'them', 'as', 'they', 'left', 'the', 'field', '.', 'ron', 'gave', 'a', 'huge', 'heave', 'and', 'more', 'slugs', 'dribbled', 'down', 'his', 'front', '.', '\\\\oooh', ',', '\\\\', 'said', 'colin', ',', 'fascinated', 'and', 'raising', 'his', 'camera', '.', '\\\\can', 'you', 'hold', 'him', 'still', ',', 'harry', '?', '\\\\', '\\\\get', 'out', 'of', 'the', 'way', ',', 'colin', '!', '\\\\', 'said', 'harry', 'angrily', '.', 'he', 'and', 'hermione', 'supported', 'ron', 'out', 'of', 'the', 'stadium', 'and', 'across', 'the', 'grounds', 'toward', 'the', 'edge', 'of', 'the', 'forest', '.', '\\\\nearly', 'there', ',', 'ron', ',', '\\\\', 'said', 'hermione', 'as', 'the', 'gamekeeper', \"'\", 's', 'cabin', 'came', 'into', 'view', '.', '\\\\you', \"'\", 'll', 'be', 'all', 'right', 'in', 'a', 'minute', '-', 'almost', 'there', '-\\\\', 'they', 'were', 'within', 'twenty', 'feet', 'of', 'hagrid', \"'\", 's', 'house', 'when', 'the', 'front', 'door', 'opened', ',', 'but', 'it', 'wasn', \"'\", 't', 'hagrid', 'who', 'emerged', '.', 'gilderoy', 'lockhart', ',', 'wearing', 'robes', 'of', 'palest', 'mauve', 'today', ',', 'came', 'striding', 'out', '.', '\\\\quick', ',', 'behind', 'here', ',', '\\\\', 'harry', 'hissed', ',', 'dragging', 'ron', 'behind', 'a', 'nearby', 'bush', '.', 'hermione', 'followed', ',', 'somewhat', 'reluctantly', '.', '*113*', '*', '\\\\it', \"'\", 's', 'a', 'simple', 'matter', 'if', 'you', 'know', 'what', 'you', \"'\", 're', 'doing', '!', '\\\\', 'lockhart', 'was', 'saying', 'loudly', 'to', 'hagrid', '.', '\\\\if', 'you', 'need', 'help', ',', 'you', 'know', 'where', 'i', 'am', '!', 'i', \"'\", 'll', 'let', 'you', 'have', 'a', 'copy', 'of', 'my', 'book', '.', 'i', \"'\", 'm', 'surprised', 'you', 'haven', \"'\", 't', 'already', 'got', 'one', '-', 'i', \"'\", 'll', 'sign', 'one', 'tonight', 'and', 'send', 'it', 'over', '.', 'well', ',', 'good-bye', '!', '\\\\', 'and', 'he', 'strode', 'away', 'toward', 'the', 'castle', '.', 'harry', 'waited', 'until', 'lockhart', 'was', 'out', 'of', 'sight', ',', 'then', 'pulled', 'ron', 'out', 'of', 'the', 'bush', 'and', 'up', 'to', 'hagrid', \"'\", 's', 'front', 'door', '.', 'they', 'knocked', 'urgently', '.', 'hagrid', 'appeared', 'at', 'once', ',', 'looking', 'very', 'grumpy', ',', 'but', 'his', 'expression', 'brightened', 'when', 'he', 'saw', 'who', 'it', 'was', '.', '\\\\bin', 'wonderin', \"'\", 'when', 'you', \"'\", 'd', 'come', 'ter', 'see', 'me', '-', 'come', 'in', ',', 'come', 'in', '-', 'thought', 'you', 'mighta', 'bin', 'professor', 'lockhart', 'back', 'again', '-\\\\', 'harry', 'and', 'hermione', 'supported', 'ron', 'over', 'the', 'threshold', 'into', 'the', 'one-', 'roomed', 'cabin', ',', 'which', 'had', 'an', 'enormous', 'bed', 'in', 'one', 'corner', ',', 'a', 'fire', 'crackling', 'merrily', 'in', 'the', 'other', '.', 'hagrid', 'didn', \"'\", 't', 'seem', 'perturbed', 'by', 'ron', \"'\", 's', 'slug', 'problem', ',', 'which', 'harry', 'hastily', 'explained', 'as', 'he', 'lowered', 'ron', 'into', 'a', 'chair', '.', '\\\\better', 'out', 'than', 'in', ',', '\\\\', 'he', 'said', 'cheerfully', ',', 'plunking', 'a', 'large', 'copper', 'basin', 'in', 'front', 'of', 'him', '.', '\\\\get', \"'\", 'em', 'all', 'up', ',', 'ron', '.', '\\\\', '\\\\i', 'don', \"'\", 't', 'think', 'there', \"'\", 's', 'anything', 'to', 'do', 'except', 'wait', 'for', 'it', 'to', 'stop', ',', '\\\\', 'said', 'hermione', 'anxiously', ',', 'watching', 'ron', 'bend', 'over', 'the', 'basin', '.', '\\\\that', \"'\", 's', 'a', 'difficult', 'curse', 'to', 'work', 'at', 'the', 'best', 'of', 'times', ',', 'but', 'with', 'a', 'broken', 'wand', '-\\\\', 'hagrid', 'was', 'bustling', 'around', 'making', 'them', 'tea', '.', 'his', 'boarhound', ',', 'fang', ',', 'was', 'slobbering', 'over', 'harry', '.', '\\\\what', 'did', 'lockhart', 'want', 'with', 'you', ',', 'hagrid', '?', '\\\\', 'harry', 'asked', ',', 'scratching', 'fang', \"'\", 's', 'ears', '.', '\\\\givin', \"'\", 'me', 'advice', 'on', 'gettin', \"'\", 'kelpies', 'out', 'of', 'a', 'well', ',', '\\\\', 'growled', '*114*', 'hagrid', ',', 'moving', 'a', 'half-plucked', 'rooster', 'off', 'his', 'scrubbed', 'table', 'and', 'setting', 'down', 'the', 'teapot', '.', '\\\\like', 'i', 'don', \"'\", 'know', '.', 'an', \"'\", 'bangin', \"'\", 'on', 'about', 'some', 'banshee', 'he', 'banished', '.', 'if', 'one', 'word', 'of', 'it', 'was', 'true', ',', 'i', \"'\", 'll', 'eat', 'my', 'kettle', '.', '\\\\', 'it', 'was', 'most', 'unlike', 'hagrid', 'to', 'criticize', 'a', 'hogwarts', \"'\", 'teacher', ',', 'and', 'harry', 'looked', 'at', 'him', 'in', 'surprise', '.', 'hermione', ',', 'however', ',', 'said', 'in', 'a', 'voice', 'somewhat', 'higher', 'than', 'usual', ',', '\\\\i', 'think', 'you', \"'\", 're', 'being', 'a', 'bit', 'unfair', '.', 'professor', 'dumbledore', 'obviously', 'thought', 'he', 'was', 'the', 'best', 'man', 'for', 'the', 'job', '-\\\\', '\\\\he', 'was', 'the', 'on', \"'\", 'man', 'for', 'the', 'job', ',', '\\\\', 'said', 'hagrid', ',', 'offering', 'them', 'a', 'y', 'plate', 'of', 'treacle', 'fudge', ',', 'while', 'ron', 'coughed', 'squelchily', 'into', 'his', 'basin', '.', '\\\\an', \"'\", 'i', 'mean', 'the', 'on', \"'\", 'one', '.', 'gettin', \"'\", 'very', 'difficult', 'ter', 'find', 'anyone', 'fer', 'y', 'the', 'dark', 'arts', 'job', '.', 'people', 'aren', \"'\", 't', 'too', 'keen', 'ter', 'take', 'it', 'on', ',', 'see', '.', 'they', \"'\", 're', 'startin', \"'\", 'ter', 'think', 'it', \"'\", 's', 'jinxed', '.', 'no', 'one', \"'\", 's', 'lasted', 'long', 'fer', 'a', 'while', 'now', '.', 'so', 'tell', 'me', ',', '\\\\', 'said', 'hagrid', ',', 'jerking', 'his', 'head', 'at', 'ron', '.', '\\\\who', 'was', 'he', 'tryin', \"'\", 'ter', 'curse', '?', '\\\\', '\\\\malfoy', 'called', 'hermione', 'something', '-', 'it', 'must', \"'\", 've', 'been', 'really', 'bad', ',', 'because', 'everyone', 'went', 'wild', '.', '\\\\', '\\\\it', 'was', 'bad', ',', '\\\\', 'said', 'ron', 'hoarsely', ',', 'emerging', 'over', 'the', 'tabletop', 'looking', 'pale', 'and', 'sweaty', '.', '\\\\malfoy', 'called', 'her', '`mudblood', ',', \"'\", 'hagrid', '-\\\\', 'ron', 'dived', 'out', 'of', 'sight', 'again', 'as', 'a', 'fresh', 'wave', 'of', 'slugs', 'made', 'their', 'appearance', '.', 'hagrid', 'looked', 'outraged', '.', '\\\\he', 'didn', \"'\", '!', '\\\\', 'he', 'growled', 'at', 'hermione', '.', '\\\\he', 'did', ',', '\\\\', 'she', 'said', '.', '\\\\but', 'i', 'don', \"'\", 't', 'know', 'what', 'it', 'means', '.', 'i', 'could', 'tell', 'it', 'was', 'really', 'rude', ',', 'of', 'course', '-\\\\', '\\\\it', \"'\", 's', 'about', 'the', 'most', 'insulting', 'thing', 'he', 'could', 'think', 'of', ',', '\\\\', 'gasped', 'ron', ',', 'coming', 'back', 'up', '.', '\\\\mudblood', \"'\", 's', 'a', 'really', 'foul', 'name', 'for', 'someone', 'who', 'is', 'muggle-born', '-', 'you', 'know', ',', 'non-magic', 'parents', '.', 'there', 'are', '*115*', 'some', 'wizards', '-', 'like', 'malfoy', \"'\", 's', 'family', '-', 'who', 'think', 'they', \"'\", 're', 'better', 'than', 'everyone', 'else', 'because', 'they', \"'\", 're', 'what', 'people', 'call', 'pure-blood', '.', '\\\\', 'he', 'gave', 'a', 'small', 'burp', ',', 'and', 'a', 'single', 'slug', 'fell', 'into', 'his', 'outstretched', 'hand', '.', 'he', 'threw', 'it', 'into', 'the', 'basin', 'and', 'continued', ',', '\\\\i', 'mean', ',', 'the', 'rest', 'of', 'us', 'know', 'it', 'doesn', \"'\", 't', 'make', 'any', 'difference', 'at', 'all', '.', 'look', 'at', 'neville', 'longbottom', '-', 'he', \"'\", 's', 'pure-blood', 'and', 'he', 'can', 'hardly', 'stand', 'a', 'cauldron', 'the', 'right', 'way', 'up', '.', '\\\\', '\\\\an', \"'\", 'they', 'haven', \"'\", 't', 'invented', 'a', 'spell', 'our', 'hermione', 'can', \"'\", 'do', ',', '\\\\', 'said', 'hagrid', 'proudly', ',', 'making', 'hermione', 'go', 'a', 'brilliant', 'shade', 'of', 'magenta', '.', '\\\\it', \"'\", 's', 'a', 'disgusting', 'thing', 'to', 'call', 'someone', ',', '\\\\', 'said', 'ron', ',', 'wiping', 'his', 'sweaty', 'brow', 'with', 'a', 'shaking', 'hand', '.', '\\\\dirty', 'blood', ',', 'see', '.', 'common', 'blood', '.', 'it', \"'\", 's', 'ridiculous', '.', 'most', 'wizards', 'these', 'days', 'are', 'half-blood', 'anyway', '.', 'if', 'we', 'hadn', \"'\", 't', 'married', 'muggles', 'we', \"'\", 'd', \"'\", 've', 'died', 'out', '.', '\\\\', 'he', 'retched', 'and', 'ducked', 'out', 'of', 'sight', 'again', '.', '\\\\well', ',', 'i', 'don', \"'\", 'blame', 'yeh', 'fer', 'tryin', \"'\", 'ter', 'curse', 'him', ',', 'ron', ',', '\\\\', 'said', 'hagrid', 'loudly', 'over', 'the', 'thuds', 'of', 'more', 'slugs', 'hitting', 'the', 'basin', '.', '\\\\bu', \"'\", 'maybe', 'it', 'was', 'a', 'good', 'thing', 'yer', 'wand', 'backfired', '.', \"'\", 'spect', 'lucius', 'malfoy', 'would', \"'\", 've', 'come', 'marchin', \"'\", 'up', 'ter', 'school', 'if', 'yeh', \"'\", 'd', 'cursed', 'his', 'son', '.', 'least', 'yer', 'not', 'in', 'trouble', '.', '\\\\', 'harry', 'would', 'have', 'pointed', 'out', 'that', 'trouble', 'didn', \"'\", 't', 'come', 'much', 'worse', 'than', 'having', 'slugs', 'pouring', 'out', 'of', 'your', 'mouth', ',', 'but', 'he', 'couldn', \"'\", 't', 'hagrid', \"'\", 's', 'treacle', 'fudge', 'had', 'cemented', 'his', 'jaws', 'together', '.', '\\\\harry', ',', '\\\\', 'said', 'hagrid', 'abruptly', 'as', 'though', 'struck', 'by', 'a', 'sudden', 'thought', '.', '\\\\gotta', 'bone', 'ter', 'pick', 'with', 'yeh', '.', 'i', \"'\", 've', 'heard', 'you', \"'\", 've', 'bin', 'givin', \"'\", 'out', 'signed', 'photos', '.', 'how', 'come', 'i', 'haven', \"'\", 't', 'got', 'one', '?', '\\\\', 'furious', ',', 'harry', 'wrenched', 'his', 'teeth', 'apart', '.', '\\\\i', 'have', 'not', 'been', 'giving', 'out', 'signed', 'photos', ',', '\\\\', 'he', 'said', 'hotly', '.', '\\\\if', 'lockhart', \"'\", 's', 'still', 'spreading', 'that', 'around', '-\\\\', '*116*', 'but', 'then', 'he', 'saw', 'that', 'hagrid', 'was', 'laughing', '.', '\\\\i', \"'\", 'm', 'on', \"'\", 'y', 'jokin', \"'\", ',', '\\\\', 'he', 'said', ',', 'patting', 'harry', 'genially', 'on', 'the', 'back', 'and', 'sending', 'him', 'face', 'first', 'into', 'the', 'table', '.', '\\\\i', 'knew', 'yeh', 'hadn', \"'\", 't', 'really', '.', 'i', 'told', 'lockhart', 'yeh', 'didn', \"'\", 'need', 'teh', '.', 'yer', 'more', 'famous', 'than', 'him', 'without', 'tryin', \"'\", '.', '\\\\', '\\\\bet', 'he', 'didn', \"'\", 't', 'like', 'that', ',', '\\\\', 'said', 'harry', ',', 'sitting', 'up', 'and', 'rubbing', 'his', 'chin', '.', '\\\\don', \"'\", 'think', 'he', 'did', ',', '\\\\', 'said', 'hagrid', ',', 'his', 'eyes', 'twinkling', '.', '\\\\an', \"'\", 'then', 'i', 'told', 'him', 'id', 'never', 'read', 'one', 'o', \"'\", 'his', 'books', 'an', \"'\", 'he', 'decided', 'ter', 'go', '.', 'treacle', 'fudge', ',', 'ron', '?', '\\\\', 'he', 'added', 'as', 'ron', 'reappeared', '.', '\\\\no', 'thanks', ',', '\\\\', 'said', 'ron', 'weakly', '.', '\\\\better', 'not', 'risk', 'it', '.', '\\\\', '\\\\come', 'an', \"'\", 'see', 'what', 'i', \"'\", 've', 'bin', 'growin', \"'\", ',', '\\\\', 'said', 'hagrid', 'as', 'harry', 'and', 'hermione', 'finished', 'the', 'last', 'of', 'their', 'tea', '.', 'in', 'the', 'small', 'vegetable', 'patch', 'behind', 'hagrid', \"'\", 's', 'house', 'were', 'a', 'dozen', 'of', 'the', 'largest', 'pumpkins', 'harry', 'had', 'ever', 'seen', '.', 'each', 'was', 'the', 'size', 'of', 'a', 'large', 'boulder', '.', '\\\\gettin', \"'\", 'on', 'well', ',', 'aren', \"'\", 't', 'they', '?', '\\\\', 'said', 'hagrid', 'happily', '.', '\\\\fer', 'the', 'halloween', 'feast', '.', '.', '.', 'should', 'be', 'big', 'enough', 'by', 'then', '.', '\\\\', '\\\\what', \"'\", 've', 'you', 'been', 'feeding', 'them', '?', '\\\\', 'said', 'harry', '.', 'hagrid', 'looked', 'over', 'his', 'shoulder', 'to', 'check', 'that', 'they', 'were', 'alone', '.', '\\\\well', ',', 'i', \"'\", 've', 'bin', 'givin', \"'\", 'them', '-', 'you', 'know', '-', 'a', 'bit', 'o', \"'\", 'help', '-\\\\', 'harry', 'noticed', 'hagrid', \"'\", 's', 'flowery', 'pink', 'umbrella', 'leaning', 'against', 'the', 'back', 'wall', 'of', 'the', 'cabin', '.', 'harry', 'had', 'had', 'reason', 'to', 'believe', 'before', 'now', 'that', 'this', 'umbrella', 'was', 'not', 'all', 'it', 'looked', 'in', 'fact', ',', 'he', 'had', 'the', 'strong', 'impression', 'that', 'hagrid', \"'\", 's', 'old', 'school', 'wand', 'was', 'concealed', 'inside', 'it', '.', 'hagrid', 'wasn', \"'\", 't', 'supposed', 'to', 'use', 'magic', '.', 'he', 'had', 'been', 'expelled', 'from', 'hogwarts', 'in', 'his', 'third', 'year', ',', 'but', 'harry', 'had', 'never', 'found', 'out', 'why', '-any', 'mention', 'of', 'the', 'matter', 'and', 'hagrid', 'would', 'clear', 'his', '*117*', 'throat', 'loudly', 'and', 'become', 'mysteriously', 'deaf', 'until', 'the', 'subject', 'was', 'changed', '.', '\\\\an', 'engorgement', 'charm', ',', 'i', 'suppose', '?', '\\\\', 'said', 'hermione', ',', 'halfway', 'between', 'disapproval', 'and', 'amusement', '.', '\\\\well', ',', 'you', \"'\", 've', 'done', 'a', 'good', 'job', 'on', 'them', '.', '\\\\', '\\\\that', \"'\", 's', 'what', 'yer', 'little', 'sister', 'said', ',', '\\\\', 'said', 'hagrid', ',', 'nodding', 'at', 'ron', '.', '\\\\met', 'her', 'jus', \"'\", 'yesterday', '.', '\\\\', 'hagrid', 'looked', 'sideways', 'at', 'harry', ',', 'his', 'beard', 'twitching', '.', '\\\\said', 'she', 'was', 'jus', \"'\", 'lookin', \"'\", 'round', 'the', 'grounds', ',', 'but', 'i', 'reckon', 'she', 'was', 'hopin', \"'\", 'she', 'might', 'run', 'inter', 'someone', 'else', 'at', 'my', 'house', '.', '\\\\', 'he', 'winked', 'at', 'harry', '.', '\\\\if', 'yeh', 'ask', 'me', ',', 'she', 'wouldn', \"'\", 'say', 'no', 'ter', 'a', 'signed', '-\\\\', '\\\\oh', ',', 'shut', 'up', ',', '\\\\', 'said', 'harry', '.', 'ron', 'snorted', 'with', 'laughter', 'and', 'the', 'ground', 'was', 'sprayed', 'with', 'slugs', '.', '\\\\watch', 'it', '!', '\\\\', 'hagrid', 'roared', ',', 'pulling', 'ron', 'away', 'from', 'his', 'precious', 'pumpkins', '.', 'it', 'was', 'nearly', 'lunchtime', 'and', 'as', 'harry', 'had', 'only', 'had', 'one', 'bit', 'of', 'treacle', 'fudge', 'since', 'dawn', ',', 'he', 'was', 'keen', 'to', 'go', 'back', 'to', 'school', 'to', 'eat', '.', 'they', 'said', 'good-bye', 'to', 'hagrid', 'and', 'walked', 'back', 'up', 'to', 'the', 'castle', ',', 'ron', 'hiccoughing', 'occasionally', ',', 'but', 'only', 'bringing', 'up', 'two', 'very', 'small', 'slugs', '.', 'they', 'had', 'barely', 'set', 'foot', 'in', 'the', 'cool', 'entrance', 'hall', 'when', 'a', 'voice', 'rang', 'out', ',', '\\\\there', 'you', 'are', ',', 'potter', '-', 'weasley', '.', '\\\\', 'professor', 'mcgonagall', 'was', 'walking', 'toward', 'them', ',', 'looking', 'stern', '.', '\\\\you', 'will', 'both', 'do', 'your', 'detentions', 'this', 'evening', '.', '\\\\', '\\\\what', \"'\", 're', 'we', 'doing', ',', 'professor', '?', '\\\\', 'said', 'ron', ',', 'nervously', 'suppressing', 'a', 'burp', '.', '\\\\you', 'will', 'be', 'polishing', 'the', 'silver', 'in', 'the', 'trophy', 'room', 'with', 'mr', '.', 'filch', ',', '\\\\', 'said', 'professor', 'mcgonagall', '.', '\\\\and', 'no', 'magic', ',', 'weasley', '-', 'elbow', 'grease', '.', '\\\\', '*118*', 'ron', 'gulped', '.', 'argus', 'filch', ',', 'the', 'caretaker', ',', 'was', 'loathed', 'by', 'every', 'student', 'in', 'the', 'school', '.', '\\\\and', 'you', ',', 'potter', ',', 'will', 'be', 'helping', 'professor', 'lockhart', 'answer', 'his', 'fan', 'mail', ',', '\\\\', 'said', 'professor', 'mcgonagall', '.', '\\\\oh', 'n', '-', 'professor', ',', 'can', \"'\", 't', 'i', 'go', 'and', 'do', 'the', 'trophy', 'room', ',', 'too', '?', '\\\\', 'said', 'harry', 'desperately', '.', '\\\\certainly', 'not', ',', '\\\\', 'said', 'professor', 'mcgonagall', ',', 'raising', 'her', 'eyebrows', '.', '\\\\professor', 'lockhart', 'requested', 'you', 'particularly', '.', 'eight', 'o', \"'\", 'clock', 'sharp', ',', 'both', 'of', 'you', '.', '\\\\', 'harry', 'and', 'ron', 'slouched', 'into', 'the', 'great', 'hall', 'in', 'states', 'of', 'deepest', 'gloom', ',', 'hermione', 'behind', 'them', ',', 'wearing', 'a', 'well-you-did-break-school-', 'rules', 'sort', 'of', 'expression', '.', 'harry', 'didn', \"'\", 't', 'enjoy', 'his', 'shepherd', \"'\", 's', 'pie', 'as', 'much', 'as', 'he', \"'\", 'd', 'thought', '.', 'both', 'he', 'and', 'ron', 'felt', 'they', \"'\", 'd', 'got', 'the', 'worse', 'deal', '.', '\\\\filch', \"'\", 'll', 'have', 'me', 'there', 'all', 'night', ',', '\\\\', 'said', 'ron', 'heavily', '.', '\\\\no', 'magic', '!', 'there', 'must', 'be', 'about', 'a', 'hundred', 'cups', 'in', 'that', 'room', '.', 'i', \"'\", 'm', 'no', 'good', 'at', 'muggle', 'cleaning', '.', '\\\\', '\\\\i', \"'\", 'd', 'swap', 'anytime', ',', '\\\\', 'said', 'harry', 'hollowly', '.', '\\\\i', \"'\", 've', 'had', 'loads', 'of', 'practice', 'with', 'the', 'dursleys', '.', 'answering', 'lockhart', \"'\", 's', 'fan', 'mail', '.', '.', '.', 'he', \"'\", 'll', 'be', 'a', 'nightmare', '.', '.', '.', '.', '.', '.', 'saturday', 'afternoon', 'seemed', 'to', 'melt', 'away', ',', 'and', 'in', 'what', 'seemed', 'like', 'no', 'time', ',', 'it', 'was', 'five', 'minutes', 'to', 'eight', ',', 'and', 'harry', 'was', 'dragging', 'his', 'feet', 'along', 'the', 'second-floor', 'corridor', 'to', 'lockhart', \"'\", 's', 'office', '.', 'he', 'gritted', 'his', 'teeth', 'and', 'knocked', '.', 'the', 'door', 'flew', 'open', 'at', 'once', '.', 'lockhart', 'beamed', 'down', 'at', 'him', '.', '\\\\ah', ',', 'here', \"'\", 's', 'the', 'scalawag', '!', '\\\\', 'he', 'said', '.', '\\\\come', 'in', ',', 'harry', ',', 'come', 'in', '-\\\\', 'shining', 'brightly', 'on', 'the', 'walls', 'by', 'the', 'light', 'of', 'many', 'candles', 'were', 'countless', 'framed', 'photographs', 'of', 'lockhart', '.', 'he', 'had', 'even', 'signed', 'a', 'few', 'of', 'them', '.', 'another', 'large', 'pile', 'lay', 'on', 'his', 'desk', '.', '\\\\you', 'can', 'address', 'the', 'envelopes', '!', '\\\\', 'lockhart', 'told', 'harry', ',', 'as', 'though', 'this', 'was', 'a', 'huge', 'treat', '.', '\\\\this', 'first', 'one', \"'\", 's', 'to', 'gladys', 'gudgeon', ',', 'bless', 'her', '-', 'huge', 'fan', 'of', 'mine', '-\\\\', 'the', 'minutes', 'snailed', 'by', '.', 'harry', 'let', 'lockhart', \"'\", 's', 'voice', 'wash', 'over', 'him', ',', 'occasionally', 'saying', ',', '\\\\mmm\\\\', 'and', '\\\\right\\\\', 'and', '\\\\yeah', '.', '\\\\', 'now', 'and', 'then', 'he', 'caught', 'a', 'phrase', 'like', ',', '\\\\fame', \"'\", 's', 'a', 'fickle', 'friend', ',', 'harry', ',', '\\\\', 'or', '\\\\celebrity', 'is', 'as', 'celebrity', 'does', ',', 'remember', 'that', '.', '\\\\', 'the', 'candles', 'burned', 'lower', 'and', 'lower', ',', 'making', 'the', 'light', 'dance', 'over', 'the', 'many', 'moving', 'faces', 'of', 'lockhart', 'watching', 'him', '.', 'harry', 'moved', 'his', 'aching', 'hand', 'over', 'what', 'felt', 'like', 'the', 'thousandth', 'envelope', ',', 'writing', 'out', 'veronica', 'smethley', \"'\", 's', 'address', '.', 'it', 'must', 'be', 'nearly', 'time', 'to', 'leave', ',', 'harry', 'thought', 'miserably', ',', 'please', 'let', 'it', 'be', 'nearly', 'time', '.', '.', '.', 'and', 'then', 'he', 'heard', 'something', '-', 'something', 'quite', 'apart', 'from', 'the', 'spitting', 'of', 'the', 'dying', 'candles', 'and', 'lockhart', \"'\", 's', 'prattle', 'about', 'his', 'fans', '.', 'it', 'was', 'a', 'voice', ',', 'a', 'voice', 'to', 'chill', 'the', 'bone', 'marrow', ',', 'a', 'voice', 'of', 'breathtaking', ',', 'ice-cold', 'venom', '.', '\\\\come', '.', '.', '.', 'come', 'to', 'me', '.', '.', '.', '.', 'let', 'me', 'rip', 'you', '.', '.', '.', '.', 'let', 'me', 'tear', 'you', '.', '.', '.', '.', 'let', 'me', 'kill', 'you', '.', '.', '.', '.', '\\\\', 'harry', 'gave', 'a', 'huge', 'jump', 'and', 'a', 'large', 'lilac', 'blot', 'appeared', 'on', 'veronica', 'smethley', \"'\", 's', 'street', '.', '\\\\what', '?', '\\\\', 'he', 'said', 'loudly', '.', '\\\\i', 'know', '!', '\\\\', 'said', 'lockhart', '.', '\\\\six', 'solid', 'months', 'at', 'the', 'top', 'of', 'the', 'best-', 'seller', 'list', '!', 'broke', 'all', 'records', '!', '\\\\', '\\\\no', ',', '\\\\', 'said', 'harry', 'frantically', '.', '\\\\that', 'voice', '!', '\\\\', '\\\\sorry', '?', '\\\\', 'said', 'lockhart', ',', 'looking', 'puzzled', '.', '\\\\what', 'voice', '?', '\\\\', '\\\\that', '-', 'that', 'voice', 'that', 'said', '-', 'didn', \"'\", 't', 'you', 'hear', 'it', '?', '\\\\', 'lockhart', 'was', 'looking', 'at', 'harry', 'in', 'high', 'astonishment', '.', '@7@2']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['train'][23]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], min_freq=3)\n",
    "vocab.insert_token('<unk>', 0)\n",
    "vocab.insert_token('<eos>', 1)\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7761\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<eos>', '.', ',', 'the', \"'\", '\\\\', 'and', 'to', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save vocab\n",
    "def save_vocab(vocab, path):\n",
    "    import pickle\n",
    "    output = open(path, 'wb')\n",
    "    pickle.dump(vocab, output)\n",
    "    output.close()\n",
    "\n",
    "save_vocab(vocab, './models/vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### . Prepare the batch loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        if example['tokens']:\n",
    "            tokens = example['tokens'].append('<eos>')\n",
    "            tokens = [vocab[token] for token in example['tokens']]\n",
    "            data.extend(tokens)\n",
    "    data = torch.LongTensor(data)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = data.view(batch_size, num_batches) #view vs. reshape (whether data is contiguous)\n",
    "    return data #[batch size, seq len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'],  vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4603])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps\n",
    "\n",
    "##### **Tokenization**: \n",
    "* The **get_tokenizer** function from torchtext is used to create a basic English tokenizer. This tokenizer breaks down text into individual words.\n",
    "* *lambda* function is defined to apply tokenization to each example in the dataset. The resulting tokens are stored in a new field named **tokens**.\n",
    "* The *map* function is used to apply this tokenization function to each example in the dataset, and the original text column is removed.\n",
    "\n",
    "##### **Numericalizing**:\n",
    "* The vocabulary (vocab) is built from the tokenized training dataset. It assigns a unique numerical index to each token that appears at least three times (min_freq=3).\n",
    "* Two special tokens, <unk> (unknown) and <eos> (end of sequence), are inserted into the vocabulary at indices 0 and 1, respectively.\n",
    "* The default index for the vocabulary is set to the index of <unk>.\n",
    "\n",
    "##### **get_data** method:\n",
    "* This method converts the tokenized dataset into a format suitable for training a language model.\n",
    "* For each example in the dataset, the tokens are retrieved, and <eos> is appended to represent the end of the sequence.\n",
    "* The tokens are then numericalized using the vocabulary **vocab**, and the resulting indices are added to the **data** list.\n",
    "* The list of indices is converted to a PyTorch LongTensor **torch.LongTensor**.\n",
    "* The data is reshaped into batches of size **batch_size**, and the function returns the processed data.\n",
    "\n",
    "**get_data** function is used to preprocess the tokenized datasets for training, validation, and testing.\n",
    "The resulting train_data, valid_data, and test_data are batches of numericalized sequences ready for input to the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim    = hid_dim\n",
    "        self.emb_dim    = emb_dim\n",
    "        \n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #We\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim,   \n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #Wh\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "        \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach() #not to be used for gradient computation\n",
    "        cell   = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch_size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src)) #harry potter is\n",
    "        #embedding: [batch-size, seq len, emb dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        #ouput: [batch size, seq len, hid dim]\n",
    "        #hidden: [num_layers * direction, seq len, hid_dim]\n",
    "        output = self.dropout(output)\n",
    "        prediction =self.fc(output)\n",
    "        #prediction: [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                # 400 in the paper\n",
    "hid_dim = 1024                # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 32,695,889 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model      = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "**LSTMLanguageModel**, is an LSTM-based language model implemented using PyTorch.\n",
    "The LSTM model contains following layers:\n",
    "* **Embedding Layer:** The input of this layer is -- and the output is embedding vector of dimension **emb_dim** which in our case is 1024\n",
    "* **LSTM Layer:** This layer takes emb_dim(1024) as input and outputs Hidden states with a dimension of hid_dim for each time step in the sequence.\n",
    "\n",
    "    Parameters for this layer are: \n",
    "    * Number of layers: **num_layers**(2)\n",
    "    * Hidden state dimension: **hid_dim**(1024)\n",
    "    * Dropout rate: **dropout_rate**(0.65)\n",
    "\n",
    "    Weights for this layer are initialized uniformly within the range [-init_range_other, init_range_other]\n",
    "* **Dropout Layer:** This layer is applied to discard some of the output from LSTM layer by randomly setting some of the LSTM output values to 0. Main purpose of this layer is to intoduce regularization. The rate of the outputs to be set as 0 is determined by **droupout_rate**(0.65).\n",
    "* **Linear (Fully Connected) Layer:** Finally in this layer, output from LSTM layer is fitted which has dimension **hiden_dim**(1024) and this give the score to each word in vocabulary. Here weights are initialized uniformly within the range [-init_range_other, init_range_other] and the bias is set to zero.\n",
    "\n",
    "The model has a total of 32,695,889 trainable parameters, including the weights and biases in the embedding layer, LSTM layer, linear layer, and other parameters.\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, seq len]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process\n",
    "\n",
    "##### LSTM model's major methods during training\n",
    "\n",
    "* **Hidden State Initialization:**\n",
    "The **init_hidden** method is resposible for initialising the hidden state. This method sets hidden state and cell state fro LSTM layer to 0.\n",
    "\n",
    "* **Forward Method:**\n",
    "Forward method takes batch of token sequences  and an initial hidden state (hidden) as input. It then embeds the input tokens using the embedding layer and applies dropout to the embedded sequence. Atter that it passes the sequence through the LSTM layer to obtain hidden states.\n",
    "It then applies dropout to the LSTM output and finally feeds the output through a linear layer to obtain predictions for the next tokens. This method returns the predictions as well as the hidden state.\n",
    "\n",
    "* **Detaching Hidden State:**\n",
    "The **detach_hidden** method is used to detach the hidden states from the computation graph for purposes of gradient computation during training.\n",
    "\n",
    "\n",
    "##### Train Method\n",
    "* **Loading Data:** Data is loaded to the model in batch with **num_batches**. The batches that are not in multiple of **seq_len** are dropped to maintain sequence length.\n",
    "\n",
    "* **Training Loop:** In the whole traing iteration following steps are carried out..\n",
    "    * Training data is iterated in the batch of sequence length\n",
    "    * Gradient of model parameters is set to 0\n",
    "    * Hidden state is detached from the computation for better accuracy\n",
    "    * **get_batch** method is called to get the batch of input and target sequences\n",
    "    * Data is transferred to specified device (GPU or CPU) for processing\n",
    "    * Forward pass is made to get the prediction and update the hidden state\n",
    "    * Loss of prediction is calculated given target\n",
    "    * Backpropogation is performed\n",
    "    * **clip_grad_norm** method from torch.nn.utils is used to prevent exploding gradient\n",
    "    * Model parameters are updated \n",
    "    * Loss for the epoch is accumulated\n",
    "\n",
    "* **Validation**: Similary **evaluate** method is used to get validation loss\n",
    "\n",
    "* Learning rate is updated using learning rate scheduler\n",
    "* Finally the model state where the validation loss is best is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 555.328\n",
      "\tValid Perplexity: 372.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 313.069\n",
      "\tValid Perplexity: 199.907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 170.831\n",
      "\tValid Perplexity: 134.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 123.905\n",
      "\tValid Perplexity: 116.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 103.301\n",
      "\tValid Perplexity: 106.414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 91.384\n",
      "\tValid Perplexity: 100.587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 82.858\n",
      "\tValid Perplexity: 98.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 76.439\n",
      "\tValid Perplexity: 94.981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 71.287\n",
      "\tValid Perplexity: 90.966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 67.042\n",
      "\tValid Perplexity: 91.343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 62.420\n",
      "\tValid Perplexity: 87.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 60.094\n",
      "\tValid Perplexity: 86.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 58.228\n",
      "\tValid Perplexity: 86.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 56.545\n",
      "\tValid Perplexity: 85.366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 54.964\n",
      "\tValid Perplexity: 84.586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 53.393\n",
      "\tValid Perplexity: 84.209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 52.089\n",
      "\tValid Perplexity: 83.983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 50.777\n",
      "\tValid Perplexity: 83.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 49.515\n",
      "\tValid Perplexity: 83.619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 48.338\n",
      "\tValid Perplexity: 83.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 47.213\n",
      "\tValid Perplexity: 82.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.143\n",
      "\tValid Perplexity: 82.101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 45.221\n",
      "\tValid Perplexity: 82.255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 43.692\n",
      "\tValid Perplexity: 81.926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 43.047\n",
      "\tValid Perplexity: 81.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 42.278\n",
      "\tValid Perplexity: 81.495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 41.905\n",
      "\tValid Perplexity: 81.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 41.425\n",
      "\tValid Perplexity: 81.793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 41.265\n",
      "\tValid Perplexity: 81.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 41.032\n",
      "\tValid Perplexity: 81.940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 41.006\n",
      "\tValid Perplexity: 81.890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.952\n",
      "\tValid Perplexity: 81.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.974\n",
      "\tValid Perplexity: 81.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.965\n",
      "\tValid Perplexity: 81.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.923\n",
      "\tValid Perplexity: 81.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.960\n",
      "\tValid Perplexity: 81.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.991\n",
      "\tValid Perplexity: 81.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.969\n",
      "\tValid Perplexity: 81.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 41.019\n",
      "\tValid Perplexity: 81.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.951\n",
      "\tValid Perplexity: 81.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.939\n",
      "\tValid Perplexity: 81.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.997\n",
      "\tValid Perplexity: 81.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.999\n",
      "\tValid Perplexity: 81.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.905\n",
      "\tValid Perplexity: 81.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.904\n",
      "\tValid Perplexity: 81.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.956\n",
      "\tValid Perplexity: 81.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.906\n",
      "\tValid Perplexity: 81.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 41.002\n",
      "\tValid Perplexity: 81.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.945\n",
      "\tValid Perplexity: 81.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.952\n",
      "\tValid Perplexity: 81.892\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "seq_len  = 50 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save whole model\n",
    "torch.save(model, 'best-val-lstm_lm.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 79.847\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "harry potter is going to get past fluffy . \\ \\maybe the dementors are back to the quidditch world cup , \\ said hagrid . \\i ' m sure he ' s a\n",
      "\n",
      "0.7\n",
      "harry potter is . he was still getting over on the grounds , and they were looking after a long walk against the dark arts . ron had no impression that he was\n",
      "\n",
      "0.75\n",
      "harry potter is . he was still getting over on the grounds , and they were looking after a long walk against the dark arts . ron had no impression that he was\n",
      "\n",
      "0.8\n",
      "harry potter is . he was still getting over on the grounds , and they were looking after a long walk against the dark arts . ron had no impression that he was\n",
      "\n",
      "1.0\n",
      "harry potter is already to get past anything like strange , , \\ he added , after a quarter of terror . hagrid thrust his bed , which he looked down at the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Harry Potter is '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
